<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <br>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="Global Prior Meets Local Consistency: Dual-Memory Augmented Vision-Language-Action Model for Efficient Robotic Manipulation">
    <title>Global Prior Meets Local Consistency: Dual-Memory Augmented Vision-Language-Action Model for Efficient Robotic Manipulation</title>
    <script>

    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">Global Prior Meets Local Consistency: Dual-Memory Augmented Vision-Language-Action Model for Efficient Robotic Manipulation</h1>
                        
                        <span class="author-block">
                            <a target="_blank"
                                href="https://scholar.google.com/citations?user=TDBF2UoAAAAJ&hl=en&oi=ao">Zaijing&#160;Li</a><sup>1
                                2</sup>,
                            <a target="_blank"
                                href="https://scholar.google.com/citations?user=rxaiRMUAAAAJ">Bing&#160;Hu</a><sup>1
                            </sup>,
                            <a target="_blank"
                                href="https://scholar.google.com/citations?user=9Vc--XsAAAAJ&hl=en&oi=ao">Rui&#160;Shao</a><sup>1&3&#9993</sup>,
                            <a target="_blank"
                                href="https://scholar.google.com/citations?user=Mpg0w3cAAAAJ&hl=en&oi=ao">Gongwei&#160;Chen</a><sup>1</sup>,
                            <br>
                            <a target="_blank"
                                href="https://scholar.google.com/citations?hl=en&user=Awsue7sAAAAJ">Dongmei&#160;Jiang</a><sup>2&#9993</sup>,
                            <a target="_blank"
                                href="https://scholar.google.com/citations?hl=en&user=FCIpXqwAAAAJ">Pengwei&#160;Xie</a><sup>4</sup>,
                            <a target="_blank"
                                href="https://scholar.google.com/citations?hl=en&user=FCJVUYgAAAAJ">Jianye&#160;HAO</a><sup>4</sup>,
                            <a target="_blank"
                                href="https://scholar.google.com/citations?hl=en&user=yywVMhUAAAAJ">Liqiang&#160;Nie</a>,
                            <div class="is-size-5 publication-authors" style="font-size: 10px;">
                                <span class="author-block"><sup>1</sup>Harbin Institute of Technology,
                                    Shenzhen&#160&#160&#160</span>
                                <span class="author-block"><sup>2</sup>Peng Cheng Laboratory, Shenzhen</span>
                                <br>
                                <span class="author-block"><sup>3</sup>Shenzhen Loop Area Institute&#160&#160&#160</span>
                                <span class="author-block"><sup>4</sup>Huawei Noah's Ark Lab</span>
                            </div>


                            <div class="is-size-5 publication-authors" style="font-size: 10px;">
                                <span class="author-block"><sup>&#9993&#160;</sup>Corresponding
                                    author&#160;&#160;</span>
                            </div>
                            <div class="column has-text-centered">
                                <div class="publication-links">
                                    <span class="link-block">
                                        <a target="_blank" href="http://arxiv.org/abs/2506.10357"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="ai ai-arxiv"></i>
                                            </span>
                                            <span>arXiv</span>
                                        </a>
                                    </span>

                                    <span class="link-block">
                                        <a target="_blank" href="https://arxiv.org/pdf/2506.10357"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-file-pdf"></i>
                                            </span>
                                            <span>PDF</span>
                                        </a>
                                    </span>
                                    <!-- Code Link. -->
                                    <span class="link-block">
                                        <a target="_blank" href="https://github.com/JiuTian-VL/OptimusVLA"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fab fa-github"></i>
                                            </span>
                                            <span>Code</span>
                                        </a>
                                    </span>
                                    <br />

                                </div>

                            </div>
                    </div>

                </div>
            </div>
        </div>
    </section>
    


    
    <section class="section" style="background-color:#efeff081">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%">
                            Hierarchical Vision–Language–Action (VLA) models have rapidly become a dominant paradigm for robotic manipulation. It typically comprising a Vision–Language backbone for perception and understanding, together with a generative policy for action generation. However, its performance is increasingly bottlenecked by the action generation proceess. (i) Low inference efficiency. A pronounced distributional gap between isotropic noise priors and target action distributions, which increases denoising steps and the incidence of infeasible samples. (ii) Poor robustness. Existing policies condition solely on the current observation, neglecting the constraint of history sequence and thus lacking awareness of task progress and temporal consistency. To address these issues, we introduce <strong>OptimusVLA</strong>, a dual-memory VLA framework with <strong>G</strong>lobal <strong>P</strong>rior <strong>M</strong>emory (<strong>GPM</strong>) and <strong>L</strong>ocal <strong>C</strong>onsistency <strong>M</strong>emory (<strong>LCM</strong>). GPM replaces Gaussian noise with task-level priors retrieved from semantically similar trajectories, thereby shortening the generative path and reducing the umber of function evaluations (NFE). LCM dynamically models executed action sequence to infer task progress and injects a learned consistency constraint that enforces temporal coherence and smoothness of trajectory. Across three simulation benchmarks, OptimusVLA consistently outperforms strong baselines: it achieves 98.6% average success rate on LIBERO, improves over &pi;<sub>0</sub> by 13.5% on CALVIN, and attains 38% average success rate on RoboTwin 2.0 Hard. In Real-World evaluation, OptimusVLA ranks best on Generalization and Long-horizon suites, surpassing &pi;<sub>0</sub> by 42.9% and 52.4%, respectively, while delivering 2.9&times; inference speedup. 
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    
    <section class="section" style="background-color:#ffffff">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3">OptimusVLA</h2>
                        <img src="assets/images/fig1.png" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto" width="800" height="700" />
                        <br>
                        <span style="font-size: 110%">
                            Overview of OptimusVLA framework. Given a task and the current observation, the Vision–Language backbone first encodes the inputs into a multimodal representation. GPM then retrieves a task-level prior based on this representation, while LBM dynamically encodes the historical action sequence to produce a consistency constraint. Finally, the flow policy denoises the initialization with an adaptive NFEs schedule to generate the action chunk.</span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- result -->
    <section class="section" style="background-color:#efeff081">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">Experiment</span></h2>
                        <span style="font-size: 110%">Real World Evaluation. 
                        </span>
                        <h1><span class="dvima"><span class="dvima">Table1: Main Result of OptimusVLA on Generalization Tasks and Long-horizon Tasks suites.</span></h1>
                        <div class="image-container">
                            <div class="image-wrapper">
                                <img src="assets/images/fig2.png" alt="" width="1200" height="1000"
                                    style="margin-left: auto; margin-right: auto" />
                            </div>
                        </div>
                        <br>
                    </div>
                </div>

            </div>
        </div>
    </section>

    <!--Conclusion-->
    <section class="section" style="background-color:#ffffff">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">Conclusion</span></h2>

                        <p style="font-size: 125%">
                            In this paper, we propose a dual-memory VLA framework, OptimusVLA, which contain Global Prior Memory (GPM) and Local Consistency Memory (LCM) for robotic manipulation. GPM replaces Gaussian noise with task-level priors retrieved from semantically similar trajectories, thereby shortening the generative path and reducing invalid samples without sacrificing generalization. LCM models short histories of executed actions to infer task progress and injects a learned consistency constraint that enforces temporal coherence. Together, GPM and LCM improve efficiency and robustness of OptimusVLA. Extensive experiments in both simulation platforms and the real world demonstrate the superior performance of OptimusVLA, together with substantially higher inference efficiency.
                        </p>

                    </div>
                </div>

            </div>
        </div>
    </section>


</body>

</html>
